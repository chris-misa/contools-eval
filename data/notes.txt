(using linux bridge)

20190107012837 (at -i 1):


last run (1477 containers) ran into bridge overflow:
```
docker: Error response from daemon: failed to create endpoint ping-container1477 on network bridge: adding interface veth700eb8e to bridge docker0 failed: exchange full.
```

There seems to be a latency spike in the 6 trials before this.


20190107134045: (~100 per run, all at -i 1)
20190107184349: (~500 per run, all at -i 1)

Seemingly linear increase in latency as containers increase from 10 to 300.



Gopher-net is not removing veths when containers are removed and the iface pool is getting saturated.
Or, gopher-net just can't deal with more the c.a. 60 containers. . .


20190108160600: (Each container pinging at 1 pps)

Since each container competes for network, greater effect on latency.



20190109005654: (Different topologies with all at -i 0)

Slight increase when traffic also goes through bridge.


20190109000513:
OVS with docker-compose integration:
0 to 60 at 1000 samples each (-i 1.0)

Hard to tell from limited sample range, but seems like the same trend. . .



20190109010656:
-i 0 from 0 to 500 containers on linux bridge network.

Seems like no increase, probably due to caching in the bridge / iptables.


20190109110515:
-i 0, 0 to 500 containers on linux bridge with other containers
pinging at -i 1.0


20190109195140:
0 to 10 extra containers all at -i 0 (linux bridge) topo: C-N
Note: all traffic goes to same remote target.
Probably should redo on three-node topology with bg traffic as extra node.


20190109220006: (wisc)
0 to 10 extra containers all at -i 0 (linux bridge) topo: C-C
Note the 'native' run is from native host context to target container
traversing NAT.
Note that background containers are still sending traffic to remote host.


20190109220141:
Redo of 0 to 20 C-N 1K pings, bg traffic to third host.


20190109221414: (wisc)
Redo of 20190109220006 with 0 to 20 extra containers, 2000 pings, otherwise same.


20190109223011:
Redo of C-C topo 0 to 20 containers, 1K pings, bg at third host.

20190109225416:
Container to container's host, 0 to 20 containers 1K pings, bg traffic at third host.


run1:
  20190109235753_CN:
  20190110012047_CH:
  20190110024314_CC:
Running all three topos with 0 to 100 containers, -i 0 on all, 2K packets,
linux bridge, etc.


20190111020551:
Running 0 to 20 containers timming sys_sendto, all -i 0 linux bridge.
(on xl... nodes)

20190111105726:
Re-running sys_sendto timing code on m510 nodes, 0 to 100, linux bridge, -i 0

20190111144919:
Re-running previous experiment to check on odd dip around 30 containers. . .

20190112110438:
20190112122253: (Redoing same code)
Ten bg containers, all -i 0, testing over different number of CPUs (1 - 16)

20190113110512: (by 5)
20190113120203: (by 1)
running 0 to 10 containers, 1 - 3 CPUs (for quicker results)

20190113135216:
running 0 to 100 by 10 containers, 1 - 3 CPUs (for quicker results)


20190113210320:
running 0 100 containers recvmsg timer.


20190114004653:
20190114014511:
20190114022918:
Developing fixed sense of what native should be doing (all on 16 CPUs only)

20190114031153:
First run with fixed native samples and 0 to 100 containers by 1 (all 16 CPUs)

20190114034630:
0-100 by 10 containers over powers of two CPUs. ..

20190114184333:
20190114190440:
20190114190614:
Merged in 20190114235200:
running 0 to 100, 16 cpu with container pinning, docker brigde, all normal.


20190115011833:
20190115011955:
20190115012115:
Merged in 20190115093300:
Running same 0 to 100 experiment on half the cpus (8).

20190115114120:
Running 0 to 100 by 4 with 4 CPUs per container (all 16 used)

20190115132124:
Running 0 to 100 by 4 with 2 CPUs per container (all 16 used)

Current: (utah)

'redo4' Running 0 to 100 by 1 with 4 CPUs per container (all 16 used)

Need to take native RTT at each traffic loading -> isolate container system
from normal network stack latency.
Then, make sense of it?


Instructions per packet?

Topos (codes)
(Background traffic always from containers to third-host on LAN.)
CN: Container to network: Container to second-host on LAN
CC: Container to container.
CH: Container to container's hosting node.
ST: sendto time
RT: recvmsg time


NODES MIGHT BE RUNNING OUT OF MEMORY WITH TOO MANY CONTAINERS> > > >
Each container get a mount point and this saturates something?
